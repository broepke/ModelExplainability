# How to Ensure You Can Explain Why Your Model Makes Predictions

**Explainability** is one of the most important topics you can learn and apply in Machine Learning and Data Science. Building a model that performs well is one thing, the ability to help you and others understand **why** a model produces the outcomes it does is another.

Let's take an example. Let's say you're building a machine learning model that will predict the likelihood that a customer will purchase a product. We might have different demographic information about them, we might have information about other products they consume, and we might have marketing information about them. Simply predicting the likelihood that a customer will purchase a product is not enough, and we need to understand why they purchase.

By understanding the **key drivers** of the model, we might be able to improve sales conversion rates by focusing on those key drivers. For example, we know that people are more likely to convert during certain times of the month or times of the day; we can focus our sales efforts around those windows of time that are more productive.

Now think back to your basic statistics classes and learning about simple linear regression. We all probably remember the equation of a line: `Y = MX + B`. We utilize this equation and the coefficients to predict new values for `Y`. The same goes for many machine learning models. If we can explain how each of the features of a model affects the outcome, we can help others understand how it works. For more on simple linear regression, check out my article: [Learn Excelâ€™s Powerful Tools for Linear Regression]({filename}regression.md).  We'll cover three key ways to explain your model. Let's jump in!


1. Coefficients for Linear and Logistic Regression
2. Feature Importance
3. SHAP Values

Read more here: https://www.dataknowsall.com/explainability.html
 
